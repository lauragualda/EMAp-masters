{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução à recuperação de informações\n",
    "\n",
    "## Lista de Exercícios 3 - Recuperação probabilística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import re\n",
    "from gensim import corpora, models, similarities\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import gutenberg, mac_morpho\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "import string\n",
    "from string import punctuation\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos partir da Prática 3 de modelagem de assuntos. Vamos usar a técnica de LSI para definir um conjunto de documentos relevantes.\n",
    "\n",
    "### Exercício 1:\n",
    "A partir de um corpus à sua escolha, estime um modelo de assuntos baseado no Modelo LSI. Uma vez calculado o modelo, defina um conjunto de **documentos relevantes (${\\cal R}$)** para um assunto, como os $n$ documentos que contiverem em sua representação LSI, os maiores coeficientes para o assunto escolhido. Construa uma consulta $q$, com as dez palavras mais importantes do assunto escolhido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste exercício, vou utilizar o corpora Gutenberg, que contem obras literárias do Projeto Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\antoniohof\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\antoniohof\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textos = [gutenberg.raw(id) for id in gutenberg.fileids()]\n",
    "tok_textos = [WordPunctTokenizer().tokenize(t.lower()) for t in textos]\n",
    "\n",
    "T = nltk.TextCollection(tok_textos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "swu = stopwords.words('english')+ list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(42339 unique tokens: ['guided', 'unopen', 'whereover', 'pembroke', 'jehonathan']...)\n"
     ]
    }
   ],
   "source": [
    "dicionario = corpora.Dictionary(tok_textos)\n",
    "dicionario.save('dic_gutenberg.dict')\n",
    "print(dicionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = [dicionario.doc2bow(d, allow_update=True) for d in tok_textos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi = models.LsiModel(corpus_tfidf, id2word=dicionario, num_topics=10)\n",
    "corpus_lsi = lsi[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.565*\"\"\" + 0.185*\".\"\" + 0.179*\",\"\" + 0.143*\"flambeau\" + 0.141*\"syme\" + 0.137*\"?\"\" + 0.125*\"have\" + 0.122*\"--\" + 0.121*\"mr\" + 0.116*\"turnbull\"'),\n",
       " (1,\n",
       "  '0.425*\"haue\" + 0.256*\"macb\" + 0.197*\"ham\" + 0.190*\"bru\" + 0.175*\"vs\" + 0.158*\"vpon\" + 0.153*\"brutus\" + 0.136*\"selfe\" + 0.133*\"thee\" + 0.133*\"cassi\"'),\n",
       " (2,\n",
       "  '-0.288*\"}\" + -0.262*\"thee\" + -0.195*\"unto\" + -0.183*\"thou\" + 0.169*\"haue\" + 0.160*\"\"\" + -0.159*\"thel\" + -0.135*\"thy\" + -0.123*\"heaven\" + -0.112*\"weep\"'),\n",
       " (3,\n",
       "  '-0.309*\"elinor\" + 0.277*\"syme\" + 0.260*\"buster\" + -0.237*\"elliot\" + -0.228*\"emma\" + 0.213*\"turnbull\" + -0.204*\"mrs\" + -0.198*\"marianne\" + -0.179*\"wentworth\" + -0.175*\"harriet\"'),\n",
       " (4,\n",
       "  '-0.759*\"alice\" + -0.235*\"buster\" + -0.208*\",\\'\" + -0.205*\"!\\'\" + 0.153*\"syme\" + 0.130*\"turnbull\" + -0.129*\"duchess\" + -0.128*\"gryphon\" + -0.123*\"dormouse\" + -0.114*\"?\\'\"'),\n",
       " (5,\n",
       "  '0.687*\"buster\" + -0.291*\"alice\" + 0.231*\"joe\" + 0.205*\"blacky\" + -0.198*\"syme\" + 0.154*\"billy\" + -0.139*\"turnbull\" + 0.126*\"sammy\" + 0.125*\"otter\" + 0.115*\"chatterer\"'),\n",
       " (6,\n",
       "  '-0.360*\"}\" + -0.335*\"whale\" + -0.276*\"ahab\" + 0.224*\"alice\" + -0.185*\"stubb\" + -0.181*\"queequeg\" + 0.173*\"unto\" + 0.159*\"thel\" + 0.143*\"elinor\" + -0.142*\"starbuck\"'),\n",
       " (7,\n",
       "  '-0.462*\"unto\" + 0.270*\"thel\" + 0.178*\"weep\" + -0.155*\"israel\" + 0.152*\"lyca\" + -0.137*\"6\" + -0.132*\"9\" + -0.130*\"8\" + -0.130*\"saith\" + -0.120*\"7\"'),\n",
       " (8,\n",
       "  '-0.645*\"syme\" + 0.511*\"turnbull\" + 0.399*\"macian\" + -0.149*\"elinor\" + 0.132*\"evan\" + -0.100*\"gregory\" + -0.093*\"marianne\" + -0.088*\"marquis\" + -0.074*\"professor\" + 0.066*\"elliot\"'),\n",
       " (9,\n",
       "  '-0.528*\"elinor\" + -0.333*\"marianne\" + 0.307*\"elliot\" + 0.232*\"wentworth\" + -0.194*\"dashwood\" + -0.177*\"jennings\" + 0.171*\"anne\" + 0.171*\"syme\" + -0.157*\"turnbull\" + 0.138*\"musgrove\"')]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.show_topics(10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentre os 10 tópicos gerados, vou escolher o assunto 9 para então definir os documentos relevantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elinor  marianne  elliot  wentworth  dashwood  jennings  anne  syme  turnbull  musgrove\n"
     ]
    }
   ],
   "source": [
    "assunto = lsi.print_topic(9)\n",
    "assunto = re.sub(\"\\d+\", \"\", assunto)\n",
    "assunto = re.sub('[^0-9a-zA-Z ]', '', assunto)\n",
    "print(assunto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos fazer a busca por similaridade do assunto 9, escolhido entre os 20 listados acima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.74073998283605225), (1, -0.20728452005182377), (2, 0.46205101744232624), (3, -0.89284993885431163), (4, 0.19356452767943028), (5, -0.043317220736093121), (6, 0.40978418890312363), (7, -0.13347134476203454), (8, -0.29965867643897148), (9, -0.37081386716054776)]\n"
     ]
    }
   ],
   "source": [
    "vec_bow = dicionario.doc2bow(assunto.lower().split())\n",
    "vec_lsi = lsi[vec_bow]\n",
    "print(vec_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = similarities.MatrixSimilarity(corpus_lsi) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.80243045), (11, 0.67341632), (0, 0.64102715), (9, 0.45198274), (1, 0.41647601), (5, 0.36444372), (10, 0.20737492), (3, 0.11719015), (14, 0.0022857334), (16, 0.0014392686), (15, 0.00022700906), (8, -0.0036905892), (4, -0.028835736), (7, -0.056651626), (13, -0.064940497), (12, -0.065045677), (17, -0.14795336), (6, -0.19265674)]\n"
     ]
    }
   ],
   "source": [
    "sims = index[vec_lsi]\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lista de documentos relevantes (R):\n",
    "* defini como critério coeficiente LSI >= 0,3 (o corpora não é muito grande, se eu restringir muito teria poucos documentos relevantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R = []\n",
    "for i in sims:\n",
    "        if i[1] >= 0.3:\n",
    "            R.append(i[0])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 11, 0, 9, 1, 5]\n"
     ]
    }
   ],
   "source": [
    "print(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definindo a consulta com as 10 palavras mais importantes do assunto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = ['elinor', 'marianne', 'elliot', 'wentworth', 'dashwood', 'jennings', 'anne', 'syme', 'turnbull', 'musgrove']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 2: \n",
    "Reutilizando os índices invertidos construídos em exercícios anteriores(Booleano, e TFIDF), calcule a precisão e revocação  com a consulta $q$ e o conjunto relevante ${\\cal R}$ definidos no exercício anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consulta booleana:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "booleano = defaultdict(lambda:set([]))\n",
    "for tid,t in enumerate(tok_textos):\n",
    "    for term in t:\n",
    "        booleano[term].add(tid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 11, 1, 0, 8, 10, 12]\n"
     ]
    }
   ],
   "source": [
    "resultado_b = []\n",
    "for k in q:\n",
    "    numero = booleano[(k)]\n",
    "    for j in numero:\n",
    "        if j not in resultado_b:\n",
    "            resultado_b.append(j)\n",
    "\n",
    "print(resultado_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular a Revocação e a Precisão, precisamos encontrar a interseção entre os documentos relevantes e os documentos encontrados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 11]\n"
     ]
    }
   ],
   "source": [
    "def intersecao(R, resultado_b):\n",
    "    return list(set(R) & set(resultado_b))\n",
    "\n",
    "print(intersecao(R, resultado_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisão =  0.571 Revocação =  0.667\n"
     ]
    }
   ],
   "source": [
    "precisao = len(intersecao(R, resultado_b))/len(resultado_b)\n",
    "revocacao = len(intersecao(R, resultado_b))/len(R)\n",
    "\n",
    "print('Precisão = ', round(precisao, 3), 'Revocação = ', round(revocacao,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consulta por TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.57772935998400421, 2), (0.56084409828848547, 1), (0.33946379482129985, 8), (0.33940082240258135, 10), (0.27872767279947253, 11), (0.11088026188415426, 12), (0.11088026188415426, 0)]\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix = np.zeros((len(tok_textos),len(q)))\n",
    "for j,w in enumerate(consulta):\n",
    "    for i, d in enumerate(tok_textos):\n",
    "        tfidf_matrix[i,j] = T.tf_idf(w,d)\n",
    "        \n",
    "from numpy.linalg import norm\n",
    "MN = np.array([r/norm(r) if norm(r) !=0 else np.zeros(len(r)) for r in tfidf_matrix])\n",
    "def ordem(q):\n",
    "    return [np.dot(q,r) for r in MN]\n",
    "qv = np.array([T.tf_idf(w,consulta) for w in q])\n",
    "qv /= norm(qv)\n",
    "r = ordem(qv)\n",
    "v=filter(lambda x : x[0]!=0.0, zip(r,range(len(tok_textos))))\n",
    "\n",
    "tfidf = sorted(v, reverse=True) \n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 8, 10, 11, 12, 0]\n"
     ]
    }
   ],
   "source": [
    "resultado_tfidf = []\n",
    "for i in tfidf:\n",
    "    resultado_tfidf.append(i[1])\n",
    "    \n",
    "print(resultado_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular a Revocação e a Precisão, precisamos encontrar a interseção entre os documentos relevantes e os documentos encontrados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 11]\n"
     ]
    }
   ],
   "source": [
    "def intersecao(R, resultado_tfidf):\n",
    "    return list(set(R) & set(resultado_tfidf))\n",
    "\n",
    "print(intersecao(R, resultado_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisão =  0.571 Revocação =  0.667\n"
     ]
    }
   ],
   "source": [
    "precisao = len(intersecao(R, resultado_tfidf))/len(resultado_tfidf)\n",
    "revocacao = len(intersecao(R, resultado_tfidf))/len(R)\n",
    "\n",
    "print('Precisão = ', round(precisao, 3), 'Revocação = ', round(revocacao,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercício 3: \n",
    "Usando as definições de probabilidade de relevância apresentadas no capítulo 11 do Livro, construa uma função de recuperação probabilística usando o *log da razão de Odds* como **RSV** (retrieval status value). Calcule revocação e precisão para consulta $q$ e conjunto relevante ${\\cal R}$. Compare a probabilidade $p_t=P(x_t=1|R=1,q)$, com a o rankeamento de importância das palavras que compõem o assunto escolhido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O *log da razão de Odds* para os termos da consulta $c_t$ é dado por:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "c_t = \\log \\frac{p_t(1-u_t)}{u_t(1-p_t)} = \\log \\frac{p_t}{1-p_t} - \\log \\frac{u_t}{1-u_t}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Em que:\n",
    "\n",
    "i) $x_1 = p_t/(1-p_t)$ são os odds do termo aparecer se o documento for relevante; e\n",
    "\n",
    "ii) $x_2 = u_t/(1-u_t)$ são os odds do termo aparecer se o documento for irrelevante \n",
    "\n",
    "\n",
    "\n",
    "RSV para o documento d: $RSV = ∑ c_t$ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar a mesma consulta definida no Exercício 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = ['elinor', 'marianne', 'elliot', 'wentworth', 'dashwood', 'jennings', 'anne', 'syme', 'turnbull', 'musgrove']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "booleano = defaultdict(lambda:set([])) \n",
    "for tid,t in enumerate(tok_textos):\n",
    "    for term in t:\n",
    "        booleano[term].add(tid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista com o termo 1 para cada palavra da consulta: [-1.6094379124341005, -0.6931471805599454, 0.0, 0.0, 0.0, 0.0, 0.69314718055994518, 0.69314718055994518, 0.69314718055994518, 0.69314718055994518]                                                     Lista com o termo 2 para cada palavra da consulta: [0, 0, 0, 0, 0, 0, -1.0986122886681098, -1.0986122886681098, -1.0986122886681098, -1.0986122886681098]\n"
     ]
    }
   ],
   "source": [
    "termo1 = []\n",
    "termo2 = []\n",
    "resultado = []\n",
    "for k in q:\n",
    "    numero = booleano[(k)]\n",
    "    for j in numero:\n",
    "        if j not in resultado:\n",
    "            resultado.append(j)\n",
    "    \n",
    "    def intersecao(R, resultado):  #p_t: termo presente em doc relevante\n",
    "        return list(set(R) & set(resultado))\n",
    "    \n",
    "    x1 = np.log((len(intersecao(R, resultado))/len(R))/(1-(len(intersecao(R, resultado))/len(R))))\n",
    "    \n",
    "    termo1.append(x1)\n",
    "    \n",
    "    not_intersection = list(set(resultado).difference(R))  #u_t: termo presente em doc irrelevante\n",
    "\n",
    "    not_R = []\n",
    "    for i in sims:  #definindo o conjunto de docs não relevantes\n",
    "        if i[1] < 0.3:\n",
    "            not_R.append(i[0])   \n",
    "    if len(not_intersection) == 0:\n",
    "        x2 = 0\n",
    "    else:\n",
    "        x2 = np.log((len(not_intersection)/len(not_R)) / (1-(len(not_intersection)/len(not_R))))\n",
    "    termo2.append(x2)\n",
    "\n",
    "print('Lista com o termo 1 para cada palavra da consulta:', termo1 , '                                                    '\n",
    "      'Lista com o termo 2 para cada palavra da consulta:', termo2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.6094379124341005, -0.6931471805599454, 0.0, 0.0, 0.0, 0.0, 1.791759469228055, 1.791759469228055, 1.791759469228055, 1.791759469228055]\n"
     ]
    }
   ],
   "source": [
    "soma_ct = []\n",
    "for k in range(10):\n",
    "    ct = termo1[k] - termo2[k]\n",
    "    soma_ct.append(ct)\n",
    "    \n",
    "print(soma_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.864\n"
     ]
    }
   ],
   "source": [
    "RSV = 0\n",
    "for k in range(10):\n",
    "    RSV = RSV+soma_ct[k]\n",
    "    \n",
    "print(round(RSV,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Professor, tentei fazer seguindo os slides, mas acho que não fiz certo, então não soube continuar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 4:\n",
    "Repita o exercício 3 agora usando o modelo **Okapi BM25** para o rankeamento. Calcule precisão e revocação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Professor, tentei aprender a utilizar o modelo seguindo os slides e \n",
    "algumas implementaçòes sugeridas na internet mas não consegui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self = textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BM25 :\n",
    "    def __init__(self, fn_docs, delimiter='|') :\n",
    "        self.dictionary = corpora.Dictionary()\n",
    "        self.DF = {}\n",
    "        self.delimiter = delimiter\n",
    "        self.DocTF = []\n",
    "        self.DocIDF = {}\n",
    "        self.N = 0\n",
    "        self.DocAvgLen = 0\n",
    "        self.fn_docs = fn_docs\n",
    "        self.DocLen = []\n",
    "        self.buildDictionary()\n",
    "        self.TFIDF_Generator()\n",
    "\n",
    "    def buildDictionary(self) :\n",
    "        raw_data = []\n",
    "        for line in file(self.fn_docs) :\n",
    "            raw_data.append(line.strip().split(self.delimiter))\n",
    "        self.dictionary.add_documents(raw_data)\n",
    "\n",
    "    def TFIDF_Generator(self, base=math.e) :\n",
    "        docTotalLen = 0\n",
    "        for line in file(self.fn_docs) :\n",
    "            doc = line.strip().split(self.delimiter)\n",
    "            docTotalLen += len(doc)\n",
    "            self.DocLen.append(len(doc))\n",
    "            #print self.dictionary.doc2bow(doc)\n",
    "            bow = dict([(term, freq*1.0/len(doc)) for term, freq in self.dictionary.doc2bow(doc)])\n",
    "            for term, tf in bow.items() :\n",
    "                if term not in self.DF :\n",
    "                    self.DF[term] = 0\n",
    "                self.DF[term] += 1\n",
    "            self.DocTF.append(bow)\n",
    "            self.N = self.N + 1\n",
    "        for term in self.DF:\n",
    "            self.DocIDF[term] = math.log((self.N - self.DF[term] +0.5) / (self.DF[term] + 0.5), base)\n",
    "        self.DocAvgLen = docTotalLen / self.N\n",
    "\n",
    "    def BM25Score(self, Query=[], k1=1.5, b=0.75) :\n",
    "        query_bow = self.dictionary.doc2bow(Query)\n",
    "        scores = []\n",
    "        for idx, doc in enumerate(self.DocTF) :\n",
    "            commonTerms = set(dict(query_bow).keys()) & set(doc.keys())\n",
    "            tmp_score = []\n",
    "            doc_terms_len = self.DocLen[idx]\n",
    "            for term in commonTerms :\n",
    "                upper = (doc[term] * (k1+1))\n",
    "                below = ((doc[term]) + k1*(1 - b + b*doc_terms_len/self.DocAvgLen))\n",
    "                tmp_score.append(self.DocIDF[term] * upper / below)\n",
    "            scores.append(sum(tmp_score))\n",
    "        return scores\n",
    "\n",
    "    def TFIDF(self) :\n",
    "        tfidf = []\n",
    "        for doc in self.DocTF :\n",
    "            doc_tfidf  = [(term, tf*self.DocIDF[term]) for term, tf in doc.items()]\n",
    "            doc_tfidf.sort()\n",
    "            tfidf.append(doc_tfidf)\n",
    "        return tfidf\n",
    "\n",
    "    def Items(self) :\n",
    "        # Return a list [(term_idx, term_desc),]\n",
    "        items = self.dictionary.items()\n",
    "        items.sort()\n",
    "        return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
